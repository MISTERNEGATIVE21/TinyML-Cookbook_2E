{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up_l5PTC9wax"
      },
      "source": [
        "# Chapter 3 - Building a weather station with TensorFlow Lite for Microcontrollers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLIttRZCUJKA"
      },
      "source": [
        "### Python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D1EPWeUUNPa"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn.metrics\n",
        "import tensorflow as tf\n",
        "\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZINNrGD9RECB"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RJIJgR6Q7t-"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "MIN_SNOW_CM = 5 # Above this value, we consider it as snow\n",
        "NUM_EPOCHS = 20\n",
        "OUTPUT_DATASET_FILE = \"snow_dataset.csv\"\n",
        "TFL_MODEL_FILE = \"snow_model.tflite\"\n",
        "TFL_MODEL_HEADER_FILE = \"model.h\"\n",
        "TF_MODEL = \"snow_forecast\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mrP29FLiiKe"
      },
      "source": [
        "## Importing weather data from WorldWeatherOnline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLmg4EGROrX"
      },
      "source": [
        "### Install the www-hist package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSY57BiSLP78"
      },
      "outputs": [],
      "source": [
        "!pip install wwo-hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnKh8e2MS8sb"
      },
      "source": [
        "### Import retrieve_hist_data function from wwo-hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w-_xrp8S_49"
      },
      "outputs": [],
      "source": [
        "from wwo_hist import retrieve_hist_data # WorldWeatherOnline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuEPsg_XTDas"
      },
      "source": [
        "### Method 1: Acquire data using the www_hist Python module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOYKjRGJRm7D"
      },
      "outputs": [],
      "source": [
        "frequency=1\n",
        "api_key = 'CHANGE-ME'\n",
        "location_list = ['canazei']\n",
        "\n",
        "# retrieve_hist_data returns a list of dataframe(s)\n",
        "hist_df = retrieve_hist_data(api_key,\n",
        "                             location_list,\n",
        "                             '01-JAN-2011',\n",
        "                             '31-DEC-2020',\n",
        "                             frequency,\n",
        "                             location_label = False,\n",
        "                             export_csv = False,\n",
        "                             store_df = True)\n",
        "\n",
        "# Extract temperature, humidity and precipitation\n",
        "t_list = hist_df[0].tempC.astype(float).to_list()\n",
        "h_list = hist_df[0].humidity.astype(float).to_list()\n",
        "s_list = hist_df[0].totalSnow_cm.astype(float).to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 2: Acquire data using the Historical Weather API"
      ],
      "metadata": {
        "id": "UvUYJEBfipfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import calendar\n",
        "import requests\n",
        "\n",
        "api_key = 'CHANGE-ME'\n",
        "city = 'canazei'\n",
        "\n",
        "t_list = []\n",
        "h_list = []\n",
        "s_list = []\n",
        "\n",
        "for year in range(2011, 2021):\n",
        "  for month in range(1, 13):\n",
        "    num_days_month = calendar.monthrange(year, month)[1]\n",
        "    start_date='{year}-{month}-01'.format(year=year, month=month)\n",
        "    end_date='{year}-{month}-{last_day}'.format(year=year, month=month, last_day=num_days_month)\n",
        "\n",
        "    url_base = 'http://api.worldweatheronline.com/premium/v1/past-weather.ashx'\n",
        "    api_url = url_base + \"?key={key}&q={city}&format=json&date={start_date}&enddate={end_date}&tp=1\".format(key=api_key,\n",
        "                                                                                                            city=city,\n",
        "                                                                                                            start_date=start_date,\n",
        "                                                                                                            end_date=end_date)\n",
        "\n",
        "    print(api_url)\n",
        "\n",
        "    response = requests.get(api_url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "      json = response.json()\n",
        "\n",
        "      for x in json['data']['weather']:\n",
        "        snow_in_cm = float(x['totalSnow_cm'])\n",
        "        for y in x['hourly']:\n",
        "          t = float(y['tempC'])\n",
        "          h = float(y['humidity'])\n",
        "          t_list.append(t)\n",
        "          h_list.append(h)\n",
        "          s_list.append(snow_in_cm)"
      ],
      "metadata": {
        "id": "9mZ1zYOaYXMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZridbJUuR-jt"
      },
      "source": [
        "## Preparing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9t51laOtkA3"
      },
      "source": [
        "### Explore the extracted physical quantities in a 2D scatter chart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wImIp3R9tnW0"
      },
      "outputs": [],
      "source": [
        "t_bin_list = []\n",
        "h_bin_list = []\n",
        "\n",
        "for snow, t, h in zip(s_list, t_list, h_list):\n",
        "  if snow > MIN_SNOW_CM:\n",
        "    t_bin_list.append(t)\n",
        "    h_bin_list.append(h)\n",
        "\n",
        "plt.figure(dpi=100)\n",
        "sc = plt.scatter(t_bin_list, h_bin_list, c='#000000', label=\"Snow\")\n",
        "plt.grid(color = '#AAAAAA', linestyle = '--', linewidth = 0.5)\n",
        "plt.legend()\n",
        "plt.title(\"Snowfall\")\n",
        "plt.xlabel(\"Temperature - Â°C\")\n",
        "plt.ylabel(\"Humidity - %\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boDIPMWTSdPZ"
      },
      "source": [
        "### Generate the output labels (Yes and No)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xe6nbGpSif2"
      },
      "outputs": [],
      "source": [
        "def gen_label(snow):\n",
        "  if snow > MIN_SNOW_CM:\n",
        "    return \"Yes\"\n",
        "  else:\n",
        "    return \"No\"\n",
        "\n",
        "labels_list = []\n",
        "\n",
        "for snow, temp in zip(s_list, t_list):\n",
        "  labels_list.append(gen_label(snow))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EbC3q1qTQuJ"
      },
      "source": [
        "### Build the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV7hQfqda6Y2"
      },
      "outputs": [],
      "source": [
        "csv_header = [\"Temp0\", \"Temp1\", \"Temp2\", \"Humi0\", \"Humi1\", \"Humi2\", \"Snow\"]\n",
        "\n",
        "dataset_df = pd.DataFrame(list(zip(t_list[:-2], t_list[1:-1], t_list[2:], h_list[:-2], h_list[1:-1], h_list[2:], labels_list[2:])), columns = csv_header)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTYLr7fg_4Vm"
      },
      "source": [
        "### Balance the dataset by undersampling the majority class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2hxpmrLINEf"
      },
      "outputs": [],
      "source": [
        "df0 = dataset_df[dataset_df['Snow'] == \"No\"]\n",
        "df1 = dataset_df[dataset_df['Snow'] == \"Yes\"]\n",
        "\n",
        "nosnow_samples_old_percent = round((len(df0.index) / (len(dataset_df.index))) * 100, 2)\n",
        "snow_samples_old_percent   = round((len(df1.index) / (len(dataset_df.index))) * 100, 2)\n",
        "\n",
        "print(len(df0.index), len(df1.index))\n",
        "print(nosnow_samples_old_percent, snow_samples_old_percent)\n",
        "\n",
        "# Random subsampling of the majority class to guarantee 50% split\n",
        "if len(df1.index) < len(df0.index):\n",
        "  df0_sub = df0.sample(len(df1.index))\n",
        "  dataset_df = pd.concat([df0_sub, df1])\n",
        "else:\n",
        "  df1_sub = df1.sample(len(df0.index))\n",
        "  dataset_df = pd.concat([df1_sub, df0])\n",
        "\n",
        "df0 = dataset_df[dataset_df['Snow'] == \"No\"]\n",
        "df1 = dataset_df[dataset_df['Snow'] == \"Yes\"]\n",
        "\n",
        "nosnow_samples_new_percent = round((len(df0.index) / (len(dataset_df.index))) * 100, 2)\n",
        "snow_samples_new_percent = round((len(df1.index) / (len(dataset_df.index))) * 100, 2)\n",
        "\n",
        "# Show number of samples\n",
        "df_samples_results = pd.DataFrame.from_records(\n",
        "                [[\"% No Snow\", nosnow_samples_old_percent, nosnow_samples_new_percent],\n",
        "                [\"% Snow\", snow_samples_old_percent, snow_samples_new_percent]],\n",
        "            columns = [\"Class\", \"Before - %\", \"After - %\"], index=\"Class\").round(2)\n",
        "\n",
        "display(df_samples_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiFURGcVRkZ9"
      },
      "source": [
        "### Scale the input features with Z-score independently\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuEhj6eEJyqP"
      },
      "outputs": [],
      "source": [
        "# Get all values\n",
        "t_list = dataset_df['Temp0'].tolist()\n",
        "h_list = dataset_df['Humi0'].tolist()\n",
        "t_list = t_list + dataset_df['Temp2'].tail(2).tolist()\n",
        "h_list = h_list + dataset_df['Humi2'].tail(2).tolist()\n",
        "\n",
        "# Calculate mean and standard deviation\n",
        "t_avg = mean(t_list)\n",
        "h_avg = mean(h_list)\n",
        "t_std = std(t_list)\n",
        "h_std = std(h_list)\n",
        "print(\"COPY ME!\")\n",
        "print(\"Temperature - [MEAN, STD]  \", round(t_avg, 5), round(t_std, 5))\n",
        "print(\"Humidity - [MEAN, STD]     \", round(h_avg, 5), round(h_std, 5))\n",
        "\n",
        "# Scaling with Z-score function\n",
        "def scaling(val, avg, std):\n",
        "  return (val - avg) / (std)\n",
        "\n",
        "dataset_df['Temp0'] = dataset_df['Temp0'].apply(lambda x: scaling(x, t_avg, t_std))\n",
        "dataset_df['Temp1'] = dataset_df['Temp1'].apply(lambda x: scaling(x, t_avg, t_std))\n",
        "dataset_df['Temp2'] = dataset_df['Temp2'].apply(lambda x: scaling(x, t_avg, t_std))\n",
        "dataset_df['Humi0'] = dataset_df['Humi0'].apply(lambda x: scaling(x, h_avg, h_std))\n",
        "dataset_df['Humi1'] = dataset_df['Humi1'].apply(lambda x: scaling(x, h_avg, h_std))\n",
        "dataset_df['Humi2'] = dataset_df['Humi2'].apply(lambda x: scaling(x, h_avg, h_std))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psfnQU5ySl8P"
      },
      "source": [
        "### Visualize raw/scaled input features distributions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8HeV9JhSdM8"
      },
      "outputs": [],
      "source": [
        "t_norm_list = dataset_df['Temp0'].tolist()\n",
        "h_norm_list = dataset_df['Humi0'].tolist()\n",
        "t_norm_list = t_norm_list + dataset_df['Temp2'].tail(2).tolist()\n",
        "h_norm_list = h_norm_list + dataset_df['Humi2'].tail(2).tolist()\n",
        "\n",
        "fig, ax=plt.subplots(1,2)\n",
        "plt.subplots_adjust(wspace = 0.4)\n",
        "ax[0].set_title(\"Raw temperature\")\n",
        "ax[1].set_title(\"Raw humidity\")\n",
        "sns.histplot(t_list, ax=ax[0], kde=True)\n",
        "sns.histplot(h_list, ax=ax[1], kde=True)\n",
        "\n",
        "fig, ax=plt.subplots(1,2)\n",
        "plt.subplots_adjust(wspace = 0.5)\n",
        "sns.histplot(t_norm_list, ax=ax[0], kde=True)\n",
        "ax[0].set_title(\"Scaled temperature\")\n",
        "ax[1].set_title(\"Scaled humidity\")\n",
        "sns.histplot(h_norm_list, ax=ax[1], kde=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdYHA2tsWRue"
      },
      "source": [
        "### Export to CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8VMeyS3WVTE"
      },
      "outputs": [],
      "source": [
        "dataset_df.to_csv(OUTPUT_DATASET_FILE, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvOQA_91OTXi"
      },
      "source": [
        "## Training the ML model with TF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnZm1IWfO0R5"
      },
      "source": [
        "### Extract the input features and output labels from the dataset_df Pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wdgh78PbO88o"
      },
      "outputs": [],
      "source": [
        "f_names = dataset_df.columns.values[0:6]\n",
        "l_name  = dataset_df.columns.values[6:7]\n",
        "x = dataset_df[f_names]\n",
        "y = dataset_df[l_name]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5cHGmqmPB8j"
      },
      "source": [
        "### Encode the labels to numerical values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRj3fc4EPJOi"
      },
      "outputs": [],
      "source": [
        "labelencoder = LabelEncoder()\n",
        "labelencoder.fit(y.Snow)\n",
        "y_encoded = labelencoder.transform(y.Snow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4esjhR8nPOQv"
      },
      "source": [
        "### Split the dataset into train, validation, and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNBv7bSQPTyi"
      },
      "outputs": [],
      "source": [
        "# Split 1 (80% vs 20%)\n",
        "x_train, x_validate_test, y_train, y_validate_test = train_test_split(x, y_encoded, test_size=0.20, random_state = 1)\n",
        "# Split 2 (50% vs 50%)\n",
        "x_test, x_validate, y_test, y_validate = train_test_split(x_validate_test, y_validate_test, test_size=0.50, random_state = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvT6XrP5Pawx"
      },
      "source": [
        "### Create the model with Keras API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RsMj7kKPgAo"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(layers.Dense(12, activation='relu', input_shape=(len(f_names),)))\n",
        "model.add(layers.Dropout(0.4))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAQWbyvcPlET"
      },
      "source": [
        "### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI3vhc6IPpLb"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp_91rLWPt6Y"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7BLJQrlPxWS"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train, y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_data=(x_validate, y_validate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_9CWAsDQBsM"
      },
      "source": [
        "### Analyze the accuracy and loss after each training epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-cKFnAEQJYm"
      },
      "outputs": [],
      "source": [
        "loss_train = history.history['loss']\n",
        "loss_val   = history.history['val_loss']\n",
        "acc_train  = history.history['accuracy']\n",
        "acc_val    = history.history['val_accuracy']\n",
        "epochs     = range(1, NUM_EPOCHS + 1)\n",
        "\n",
        "def plot_train_val_history(x, y_train, y_val, type_txt):\n",
        "  plt.figure(dpi=150)\n",
        "  plt.plot(x, y_train, 'g', label='Training'+type_txt)\n",
        "  plt.plot(x, y_val, 'b', label='Validation'+type_txt)\n",
        "  plt.title('Training and Validation'+type_txt)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel(type_txt)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "plot_train_val_history(epochs, loss_train, loss_val, \"Loss\")\n",
        "plot_train_val_history(epochs, acc_train, acc_val, \"Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH5k1yxIQX9l"
      },
      "source": [
        "### Save the entire TensorFlow model as a SavedModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkfSnfTaQcaA"
      },
      "outputs": [],
      "source": [
        "model.save(TF_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssiWwrM2TO3b"
      },
      "source": [
        "## Evaluating the model effectiveness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGz7FiodrvxK"
      },
      "source": [
        "### Use the trained model to predict the output classes of the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25jxtgREr3Pm"
      },
      "outputs": [],
      "source": [
        "y_test_pred = model.predict(x_test)\n",
        "y_test_pred = (y_test_pred > 0.5).astype(\"int32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64iTnERfun1s"
      },
      "source": [
        "### Compute the confusion matrix with scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1YHpg6cu4Bp"
      },
      "outputs": [],
      "source": [
        "cm = sklearn.metrics.confusion_matrix(y_test, y_test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBkNtdfITaLP"
      },
      "source": [
        "### Visualize the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnXH1AK-TV6m"
      },
      "outputs": [],
      "source": [
        "index_names  = [\"Actual No Snow\", \"Actual Snow\"]\n",
        "column_names = [\"Predicted No Snow\", \"Predicted Snow\"]\n",
        "\n",
        "df_cm = pd.DataFrame(cm, index = index_names, columns = column_names)\n",
        "\n",
        "plt.figure(dpi=150)\n",
        "sns.heatmap(df_cm, annot=True, fmt='d', cmap=\"Blues\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVo3KjpNTmMM"
      },
      "source": [
        "### Calculate Recall, Precision, and F-score performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KewMzVMTsdm"
      },
      "outputs": [],
      "source": [
        "TN = cm[0][0]\n",
        "TP = cm[1][1]\n",
        "FN = cm[1][0]\n",
        "FP = cm[0][1]\n",
        "\n",
        "accur  = (TP + TN) / (TP + TN + FN + FP)\n",
        "precis = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "f_score = (2 * recall * precis) / (recall + precis)\n",
        "\n",
        "print(\"Accuracy:  \", round(accur, 3))\n",
        "print(\"Recall:    \", round(recall, 3))\n",
        "print(\"Precision: \", round(precis, 3))\n",
        "print(\"F-score:   \", round(f_score, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G3USDyST8XC"
      },
      "source": [
        "## Quantizing the model with TFLite converter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwVXYHLrUBpW"
      },
      "source": [
        "### Select a few hundred of samples randomly from the test dataset to calibrate the quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1z6BqPZUS-5"
      },
      "outputs": [],
      "source": [
        "def representative_data_gen():\n",
        "  data = tf.data.Dataset.from_tensor_slices(x_test)\n",
        "  for i_value in data.batch(1).take(100):\n",
        "    i_value_f32 = tf.dtypes.cast(i_value, tf.float32)\n",
        "    yield [i_value_f32]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHItQuK8UW21"
      },
      "source": [
        "### Import the TensorFlow SavedModel directory into TensorFlow Lite Converter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLzuXGy_UbbA"
      },
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(TF_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um7vdQI3Ur2P"
      },
      "source": [
        "### Initialize TensorFlow Lite converter for the 8-bit quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qs7BI6HUwVu"
      },
      "outputs": [],
      "source": [
        "converter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa9y3-hGU0H2"
      },
      "source": [
        "### Convert the model to TensorFlow Lite file format (FlatBuffers) as save it as .tflite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6vhVG09U82z"
      },
      "outputs": [],
      "source": [
        "tflite_model_quant = converter.convert()\n",
        "open(TFL_MODEL_FILE, \"wb\").write(tflite_model_quant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMYG7uZHVM2_"
      },
      "source": [
        "### Convert the TensorFlow Lite model to C-byte array with xxd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzE0vsBfVS1p"
      },
      "outputs": [],
      "source": [
        "!apt-get update && apt-get -qq install xxd\n",
        "!xxd -i $TFL_MODEL_FILE > model.h\n",
        "!sed -i 's/unsigned char/const unsigned char/g' model.h\n",
        "!sed -i 's/const/alignas(8) const/g' model.h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flA5AXTPeIws"
      },
      "source": [
        "### Get the TensorFlow model size in bytes to estimate the program memory usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB6tK_ckeLby"
      },
      "outputs": [],
      "source": [
        "size_tfl_model = len(tflite_model_quant)\n",
        "print(len(tflite_model_quant), \"bytes\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}