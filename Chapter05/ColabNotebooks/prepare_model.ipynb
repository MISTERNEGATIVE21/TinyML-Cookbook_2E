{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsZpsRh2hoOn"
      },
      "source": [
        "# Chapter 5 - Recognizing music genres with TensorFlow and the Raspberry Pi Pico\n",
        "## Part 1: Data collection and MFCCs computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnwIrmZKh2AZ"
      },
      "source": [
        "## Python libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5\n",
        "!pip install cmsisdsp==1.9.6"
      ],
      "metadata": {
        "id": "uP22fn8eQvA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "paHVNjoLGMHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "import librosa\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cmsisdsp as dsp\n",
        "import math\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "nM7JEWpcGPOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by8Rk4ush56O"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUzrdzCUM0Ga"
      },
      "outputs": [],
      "source": [
        "# Google drive top-level directory\n",
        "G_DRIVE_ROOT = '/content/drive/'\n",
        "\n",
        "# Audio sample rate\n",
        "SAMPLE_RATE = 22050\n",
        "\n",
        "# MFFC constants\n",
        "FRAME_LENGTH = 2048\n",
        "FRAME_STEP = 1024\n",
        "FFT_LENGTH = 2048\n",
        "FMIN_HZ = 20\n",
        "FMAX_HZ = SAMPLE_RATE / 2\n",
        "NUM_MEL_FREQS = 40\n",
        "NUM_MFCCS = 18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIJQJQB_jd7j"
      },
      "source": [
        "## Extracting MFCCs from audio samples with TensorFlow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB79lTAvioX7"
      },
      "source": [
        "### Implement a function to compute MFCCs with the TensorFlow signal processing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S59awBIzMXdY"
      },
      "outputs": [],
      "source": [
        "def extract_mfccs_tf(\n",
        "  ad_src,\n",
        "  ad_sample_rate,\n",
        "  num_mfccs,\n",
        "  frame_length,\n",
        "  frame_step,\n",
        "  fft_length,\n",
        "  fmin_hz,\n",
        "  fmax_hz,\n",
        "  num_mel_freqs):\n",
        "\n",
        "  n = ad_src.shape[0]\n",
        "  num_frames = int(((n - frame_length) / frame_step) + 1)\n",
        "\n",
        "  output = np.zeros(shape=(num_frames, num_mfccs))\n",
        "\n",
        "  # Iterate over each frame to get the MFCC coefficients\n",
        "  for i in range(num_frames):\n",
        "    idx_s = i * frame_step\n",
        "    idx_e = idx_s + frame_length\n",
        "    src = ad_src[idx_s:idx_e]\n",
        "\n",
        "    # Apply the Hann Window in-place\n",
        "    hann_coef = tf.signal.hann_window(frame_length)\n",
        "    hann = src * hann_coef\n",
        "\n",
        "    # Apply the RFFT\n",
        "    fft_spect = tf.signal.rfft(hann)\n",
        "\n",
        "    # Calculate the magnitude of the FFT\n",
        "    fft_mag_spect = tf.math.abs(fft_spect)\n",
        "\n",
        "    # Calculate the coefficients of Mel-weights for converting the spectrum from Hz to Mel\n",
        "    num_fft_freqs = fft_mag_spect.shape[0]\n",
        "    mel_wei_mtx = tf.signal.linear_to_mel_weight_matrix(\n",
        "      num_mel_freqs,\n",
        "      num_fft_freqs,\n",
        "      ad_sample_rate,\n",
        "      fmin_hz,\n",
        "      fmax_hz)\n",
        "\n",
        "    # Convert the spectrum to Mel\n",
        "    mel_spect = np.matmul(fft_mag_spect, mel_wei_mtx)\n",
        "\n",
        "    # Perform the log function\n",
        "    log_mel_spect = np.log(mel_spect + 1e-6)\n",
        "\n",
        "    dct = tf.signal.mfccs_from_log_mel_spectrograms(\n",
        "    log_mel_spect)\n",
        "\n",
        "    # Extract the MFFC coefficients\n",
        "    output[i] = dct[0:num_mfccs]\n",
        "\n",
        "  return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI-HuxGGLqzi"
      },
      "source": [
        "### Mount top-level Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTppFu0PFnVU"
      },
      "outputs": [],
      "source": [
        "drive.mount(G_DRIVE_ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHANGE TO DESIRED TRAIN FOLDER\n",
        "train_dir = \"drive/MyDrive/mgr_dataset\""
      ],
      "metadata": {
        "id": "GLVyxF1QQ1r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkyPFrzagIqO"
      },
      "source": [
        "### Load an audio file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvO1xMrmgO-s"
      },
      "outputs": [],
      "source": [
        "filepath = train_dir + \"/disco/disco.00002.wav\"\n",
        "ad, sr = sf.read(filepath)\n",
        "\n",
        "print('Sample rate:', sr)\n",
        "print('Sample shape:', ad.shape)\n",
        "print('Song duration:', ad.shape[0] / sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf1sXqFwIf_N"
      },
      "source": [
        "### Play one-second of audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKX4zcbkIVUb"
      },
      "outputs": [],
      "source": [
        "# Take one second of audio\n",
        "test_ad = ad[0:SAMPLE_RATE]\n",
        "\n",
        "import IPython.display as ipd\n",
        "ipd.Audio(test_ad, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4GH_0SQmpda"
      },
      "source": [
        "### Extract the MFCCs using TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6Ovv5p7mwnz"
      },
      "outputs": [],
      "source": [
        "mfccs_tf = extract_mfccs_tf(\n",
        "    test_ad,\n",
        "    SAMPLE_RATE,\n",
        "    NUM_MFCCS,\n",
        "    FRAME_LENGTH,\n",
        "    FRAME_STEP,\n",
        "    FFT_LENGTH,\n",
        "    FMIN_HZ,\n",
        "    FMAX_HZ,\n",
        "    NUM_MEL_FREQS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement a function to visualize the MFCCs as image"
      ],
      "metadata": {
        "id": "yM9aazx4TfYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "def display_mfccs(mfcc_src):\n",
        "  fig, ax = plt.subplots()\n",
        "  cax = ax.imshow(mfcc_src, interpolation='nearest', cmap=cm.gray, origin='lower')\n",
        "  ax.set_title('MFCCs')\n",
        "  plt.xlabel('Frame index - Time')\n",
        "  plt.ylabel('Coefficient index - Frequency')\n",
        "  plt.colorbar(cax)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "paIF7YWzTma-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display the MFCCs"
      ],
      "metadata": {
        "id": "0wZbC_yBTqWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_mfccs(mfccs_tf.T)"
      ],
      "metadata": {
        "id": "pgMjqrKZTel0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nay9ctlUclc8"
      },
      "source": [
        "### Implement a function to compute MFCCs algorithm with the Librosa library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pABiatQEY5cB"
      },
      "outputs": [],
      "source": [
        "def extract_mfccs_librosa(\n",
        "  ad_src,\n",
        "  ad_sample_rate,\n",
        "  num_mfccs,\n",
        "  frame_length,\n",
        "  frame_step,\n",
        "  fft_length,\n",
        "  fmin_hz,\n",
        "  fmax_hz,\n",
        "  num_mel_freqs):\n",
        "\n",
        "  return librosa.feature.mfcc(\n",
        "      y=ad_src,\n",
        "      sr=ad_sample_rate,\n",
        "      n_mfcc = num_mfccs,\n",
        "      n_fft = fft_length,\n",
        "      hop_length = frame_step,\n",
        "      win_length = frame_length,\n",
        "      center = False,\n",
        "      n_mels = num_mel_freqs,\n",
        "      fmin = fmin_hz,\n",
        "      fmax = fmax_hz)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract the MFCCs using the Librosa library\n"
      ],
      "metadata": {
        "id": "aG3QKBM8xQik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iausy829cnV0"
      },
      "outputs": [],
      "source": [
        "mfccs_librosa = extract_mfccs_librosa(\n",
        "    test_ad,\n",
        "    SAMPLE_RATE,\n",
        "    NUM_MFCCS,\n",
        "    FRAME_LENGTH,\n",
        "    FRAME_STEP,\n",
        "    FFT_LENGTH,\n",
        "    FMIN_HZ,\n",
        "    FMAX_HZ,\n",
        "    NUM_MEL_FREQS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display the MFCCs"
      ],
      "metadata": {
        "id": "OLDKNAxMcrPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_mfccs(mfccs_librosa)"
      ],
      "metadata": {
        "id": "4DzK7XXZcrq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Model deployment"
      ],
      "metadata": {
        "id": "FEO0gApqSWZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ],
      "metadata": {
        "id": "rIrejKeQSb-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Music genres\n",
        "LIST_GENRES = ['disco', 'jazz', 'metal']\n",
        "\n",
        "# Training audio length in seconds\n",
        "TRAIN_AUDIO_LENGTH_SEC = 1\n",
        "\n",
        "# Training audio length in number of samples\n",
        "TRAIN_AUDIO_LENGTH_SAMPLES = SAMPLE_RATE * TRAIN_AUDIO_LENGTH_SEC\n",
        "\n",
        "# TensorFlow model name\n",
        "TF_MODEL = 'music_genre'\n",
        "\n",
        "# TensorFlow lite model name\n",
        "TFL_MODEL_FILE = 'music_genre.tflite'"
      ],
      "metadata": {
        "id": "GpJgpEbXSbHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZlNB5weXJFT"
      },
      "source": [
        "## Computing the FFT magnitude with fixed-point arithmetic through the CMSIS-DSP Python library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X6cPdd6AcyY"
      },
      "source": [
        "### Implement a function that computes the RFFT in Q15 fixed-point using CMSIS-DSP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHMrdRx8AqIA"
      },
      "outputs": [],
      "source": [
        "def rfft_q15(src):\n",
        "  src_len = src.shape[0]\n",
        "  inst = dsp.arm_rfft_instance_q15()\n",
        "  stat = dsp.arm_rfft_init_q15(inst, src_len, 0, 1)\n",
        "  fft_q = dsp.arm_rfft_q15(inst, src)\n",
        "  return fft_q[:src_len + 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUqPPFdZA2_3"
      },
      "source": [
        "### Implement a function that computes the magnitude in Q15 fixed-point using CMSIS-DSP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgZb7GvxA8uP"
      },
      "outputs": [],
      "source": [
        "def mag_q15(src):\n",
        "  f0 = src[0],\n",
        "  fn = src[1],\n",
        "  fx = dsp.arm_cmplx_mag_q15(src[2:])\n",
        "  return np.concatenate((f0, fx, fn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI3vxk91YC5o"
      },
      "source": [
        "### Get a frame from the test audio sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wiLtuczYIcL"
      },
      "outputs": [],
      "source": [
        "src = test_ad[0:FRAME_LENGTH]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY-e--RvYjGx"
      },
      "source": [
        "### Compute the FFT magnitude using the 16-bit fixed-point arithmetic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMDU2KMWYmsB"
      },
      "outputs": [],
      "source": [
        "# Convert to Q15\n",
        "src_q15 = dsp.arm_float_to_q15(src)\n",
        "\n",
        "# Apply the RFFT. The output is Q12.4. Therefore, fewer fractional bits\n",
        "cmsis_fft_q15 = rfft_q15(src_q15)\n",
        "\n",
        "# Calculate the magnitude of the FFT. The output is Q13.3\n",
        "cmsis_fft_mag_q15 = mag_q15(cmsis_fft_q15)\n",
        "\n",
        "# Convert to float\n",
        "scale = float(1 << 3) # 8\n",
        "\n",
        "cmsis_fft_mag = cmsis_fft_mag_q15 / scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roWGbcW8f7iH"
      },
      "source": [
        "### Compute the FFT magnitude using the floating-point arithmetic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3o6dAwcgETG"
      },
      "outputs": [],
      "source": [
        "tf_fft = tf.signal.rfft(src)\n",
        "tf_fft_mag = tf.math.abs(tf_fft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRwtGU1udWmi"
      },
      "source": [
        "### Evaluate the difference stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJx1nXEvdYy0"
      },
      "outputs": [],
      "source": [
        "abs_diff = np.abs(tf_fft_mag - cmsis_fft_mag)\n",
        "print(\"Differences:\\nmin:\", np.min(abs_diff),\n",
        "      \"max:\", np.max(abs_diff),\n",
        "      \"mean:\", np.mean(abs_diff),\n",
        "      \"std:\", np.std(abs_diff))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-KOLjdpeHlM"
      },
      "source": [
        "## Implementing the MFCCs feature extraction with the CMSIS-DSP library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB4wALJi9rO0"
      },
      "source": [
        "### Implement a function to precompute the Hann window coefficients in Q15 format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khPAmUpy9622"
      },
      "outputs": [],
      "source": [
        "def gen_hann_lut_q15(frame_len):\n",
        "  hann_lut_f32 = tf.signal.hann_window(frame_len)\n",
        "  return dsp.arm_float_to_q15(hann_lut_f32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG_UMlu7B9X6"
      },
      "source": [
        "### Implement a function to precompute the Mel-weight matrix in Q15 format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZYPQ8HiCUmv"
      },
      "outputs": [],
      "source": [
        "def gen_mel_weight_mtx(sr, fmin_hz, fmax_hz, num_mel_freqs, num_fft_freqs):\n",
        "  m_f32 = tf.signal.linear_to_mel_weight_matrix(\n",
        "    num_mel_freqs,\n",
        "    num_fft_freqs,\n",
        "    sr,\n",
        "    fmin_hz,\n",
        "    fmax_hz)\n",
        "\n",
        "  m_q15 = dsp.arm_float_to_q15(m_f32)\n",
        "  # Reshape is needed because the conversion from float to q15 collapses the dimensions\n",
        "  return m_q15.reshape((m_f32.shape[0], m_f32.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbWLr0McMmHf"
      },
      "source": [
        "### Implement a function to precompute the logarithmic function with input as 16-bit fixed-point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI12qcSVM7sD"
      },
      "outputs": [],
      "source": [
        "def gen_log_lut_q(q_scale):\n",
        "  max_int16 = np.iinfo(\"int16\").max\n",
        "\n",
        "  log_lut = np.zeros(shape=(max_int16), dtype=\"int16\")\n",
        "\n",
        "  for i16 in range(0, max_int16):\n",
        "    q16 = np.array([i16,], dtype=\"int16\")\n",
        "    f_v = q16 / float(q_scale)\n",
        "    log_f = np.array(np.log(f_v + 1e-6),)\n",
        "    log_q = log_f * float(q_scale)\n",
        "    log_lut[i16] = int(log_q)\n",
        "\n",
        "  return log_lut"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHrF-ZxXIGg3"
      },
      "source": [
        "### Implement a function to precompute the DCT-weight matrix in Q15 format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huGSTwjaINil"
      },
      "outputs": [],
      "source": [
        "def gen_dct_weight_mtx(num_mel_freqs, num_mfccs):\n",
        "  mtx_q15 = np.zeros(shape=(num_mel_freqs, num_mfccs), dtype=\"int16\")\n",
        "\n",
        "  scale = np.sqrt(2.0 / float(num_mel_freqs))\n",
        "  pi_div_mel = (math.pi / num_mel_freqs)\n",
        "  for n in range(num_mel_freqs):\n",
        "    for k in range(num_mfccs):\n",
        "      v = scale * np.cos(pi_div_mel * (n + 0.5) * k)\n",
        "      v_f32 = np.array([v,], dtype=\"float32\")\n",
        "      mtx_q15[n][k] = dsp.arm_float_to_q15(v_f32)\n",
        "  return mtx_q15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-onHdSaPpJ9"
      },
      "source": [
        "### Precompute the Hann window coefficients, Mel-weight matrix, DCT-weight matrix, and logarithmic function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOnTuJR6CPLX"
      },
      "outputs": [],
      "source": [
        "# Generate the Hann Window LUT for Q15\n",
        "hann_lut_q15 = gen_hann_lut_q15(FRAME_LENGTH)\n",
        "\n",
        "# Precalculate the Mel-weight matrix in Q15 fixed-point format\n",
        "mel_wei_mtx_q15 = gen_mel_weight_mtx(SAMPLE_RATE, FMIN_HZ, FMAX_HZ, NUM_MEL_FREQS, int((FFT_LENGTH / 2) + 1))\n",
        "\n",
        "# Generate the Log LUT for Q13.3 fixed-point format\n",
        "log_lut_q13_3 = gen_log_lut_q(8)\n",
        "\n",
        "# Precalculate the DCT-II-weight matrix\n",
        "dct_wei_mtx_q15 = gen_dct_weight_mtx(NUM_MEL_FREQS, NUM_MFCCS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8sa-S34hiuM"
      },
      "source": [
        "### Show the program memory usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfm8GO7khki9"
      },
      "outputs": [],
      "source": [
        "mem_usage = 0\n",
        "mem_usage += np.size(hann_lut_q15) * 2\n",
        "mem_usage += np.size(mel_wei_mtx_q15) * 2\n",
        "mem_usage += np.size(log_lut_q13_3) * 2\n",
        "mem_usage += np.size(dct_wei_mtx_q15) * 2\n",
        "\n",
        "print(\"Program memory usage: \", mem_usage, \"bytes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement a function to compute the MFCCs feature extraction with 16-bit fixed-point arithmetic"
      ],
      "metadata": {
        "id": "CNSR5Zz6FCvz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h6Sn8GfKKzE"
      },
      "outputs": [],
      "source": [
        "def extract_mfccs_cmsis(\n",
        "  ad_src,\n",
        "  num_mfccs,\n",
        "  frame_length,\n",
        "  frame_step,\n",
        "  hann_lut_q15,\n",
        "  mel_wei_mtx_q15,\n",
        "  log_lut_q13_3,\n",
        "  dct_wei_mtx_q15):\n",
        "\n",
        "  n = ad_src.shape[0]\n",
        "  num_frames = int(((n - frame_length) / frame_step) + 1)\n",
        "\n",
        "  output = np.zeros(shape=(num_frames, num_mfccs))\n",
        "\n",
        "  # Iterate over each frame to get the MFCC coefficients\n",
        "  for i in range(num_frames):\n",
        "    idx_s = i * frame_step\n",
        "    idx_e = idx_s + frame_length\n",
        "    frame = ad_src[idx_s:idx_e]\n",
        "\n",
        "    # Convert to Q15\n",
        "    frame_q15 = dsp.arm_float_to_q15(frame)\n",
        "\n",
        "    # Apply the Hann Window. The output is still Q15\n",
        "    hann_q15 = dsp.arm_mult_q15(frame_q15, hann_lut_q15)\n",
        "\n",
        "    # Apply the RFFT. The output is Q12.4. Therefore, fewer fractional bits\n",
        "    fft_spect_q15 = rfft_q15(hann_q15)\n",
        "\n",
        "    # Calculate the magnitude of the FFT. The output is Q13.3\n",
        "    fft_mag_spect_q15 = mag_q15(fft_spect_q15)\n",
        "\n",
        "    # Convert the spectrum to Mel\n",
        "    log_mel_spect_q15 = dsp.arm_mat_vec_mult_q15(mel_wei_mtx_q15.T, fft_mag_spect_q15.T)\n",
        "\n",
        "    # Perform the log() function\n",
        "    for idx, v in enumerate(log_mel_spect_q15):\n",
        "      log_mel_spect_q15[idx] = log_lut_q13_3[v]\n",
        "\n",
        "    # Calculate the MFCCs through the DCT\n",
        "    mfccs = dsp.arm_mat_vec_mult_q15(dct_wei_mtx_q15.T, log_mel_spect_q15)\n",
        "\n",
        "    # Convert MFCCs to float\n",
        "    output[i] = mfccs.T / float(8)\n",
        "\n",
        "  return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUowbVdwQsoq"
      },
      "source": [
        "### Extract the MFCC coefficients using the CMSIS-DSP implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpFFbCBfQxtV"
      },
      "outputs": [],
      "source": [
        "mfccs_cmsis = extract_mfccs_cmsis(\n",
        "    test_ad,\n",
        "    NUM_MFCCS,\n",
        "    FRAME_LENGTH,\n",
        "    FRAME_STEP,\n",
        "    hann_lut_q15,\n",
        "    mel_wei_mtx_q15,\n",
        "    log_lut_q13_3,\n",
        "    dct_wei_mtx_q15)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display MFCCs obtained with the CMSIS-DSP library"
      ],
      "metadata": {
        "id": "t-97MyarntOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_mfccs(mfccs_cmsis.T)"
      ],
      "metadata": {
        "id": "lcRqC86snvdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display the difference"
      ],
      "metadata": {
        "id": "YhicIwBKA2_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abs_diff = np.abs(mfccs_tf - mfccs_cmsis)\n",
        "\n",
        "display_mfccs(abs_diff.T)\n",
        "\n",
        "print(\"Differences:\\nmin:\", np.min(abs_diff),\n",
        "      \"max:\", np.max(abs_diff),\n",
        "      \"mean:\", np.mean(abs_diff),\n",
        "      \"std:\", np.std(abs_diff))"
      ],
      "metadata": {
        "id": "boRTJuF1A4q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Designing and training an LSTM model"
      ],
      "metadata": {
        "id": "cSUM6zcJJURM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate the dataset and store it in a JSON file"
      ],
      "metadata": {
        "id": "0_9-vom4JjS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for genre in LIST_GENRES:\n",
        "  folder = train_dir + \"/\" + genre\n",
        "\n",
        "  list_files = os.listdir(folder)\n",
        "\n",
        "  for song in list_files:\n",
        "    filepath = folder + \"/\" + song\n",
        "\n",
        "    try:\n",
        "      ad, sr = sf.read(filepath)\n",
        "\n",
        "      # Number of splits\n",
        "      num_it = int(len(ad) / TRAIN_AUDIO_LENGTH_SAMPLES)\n",
        "\n",
        "      for i in range(num_it):\n",
        "        s0 = TRAIN_AUDIO_LENGTH_SAMPLES * i\n",
        "        s1 = s0 + TRAIN_AUDIO_LENGTH_SAMPLES\n",
        "        src_audio = ad[s0 : s1]\n",
        "\n",
        "        mfccs = extract_mfccs_cmsis(\n",
        "            src_audio,\n",
        "            NUM_MFCCS,\n",
        "            FRAME_LENGTH,\n",
        "            FRAME_STEP,\n",
        "            hann_lut_q15,\n",
        "            mel_wei_mtx_q15,\n",
        "            log_lut_q13_3,\n",
        "            dct_wei_mtx_q15)\n",
        "\n",
        "        x.append(mfccs.tolist())\n",
        "        y.append(LIST_GENRES.index(genre))\n",
        "\n",
        "    except Exception as e:\n",
        "      continue"
      ],
      "metadata": {
        "id": "_g-uF3tvRBf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert the x and y lists to NumPy arrays"
      ],
      "metadata": {
        "id": "fNXacAtpWYzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = np.array(x), np.array(y)"
      ],
      "metadata": {
        "id": "Fc1VgYP1WYGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the dataset into train (60%), validation (20%), and test (20%) datasets"
      ],
      "metadata": {
        "id": "fNYRl_ijM7a_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split 1 (60% vs 40%)\n",
        "x_train, x0, y_train, y0 = train_test_split(x, y, test_size=0.40, random_state = 1)\n",
        "# Split 2 (50% vs 50%)\n",
        "x_test, x_validate, y_test, y_validate = train_test_split(x0, y0, test_size=0.50, random_state = 3)"
      ],
      "metadata": {
        "id": "6rUNJr01cUkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Design a many-to-many LSTM model"
      ],
      "metadata": {
        "id": "JBto7a2zXZZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "input_shape = (x_train.shape[1], x_train.shape[2])\n",
        "\n",
        "norm_layer = layers.Normalization(axis=-1)\n",
        "\n",
        "# Learn mean and standard deviation from dataset\n",
        "norm_layer.adapt(x_train)\n",
        "\n",
        "input = layers.Input(shape=input_shape)\n",
        "x = norm_layer(input)\n",
        "x = layers.LSTM(32, return_sequences=True)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "x = layers.Dense(len(LIST_GENRES), activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(input, x)"
      ],
      "metadata": {
        "id": "geeERNeCNKGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize model summary"
      ],
      "metadata": {
        "id": "7fxpvDWZ39k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "o1shjkKR3_f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the many-to-many LSTM model"
      ],
      "metadata": {
        "id": "31T9tTALNWKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "model.compile(optimizer=optimiser,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "NUM_EPOCHS = 30\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_data=(x_validate, y_validate))"
      ],
      "metadata": {
        "id": "z5kLiujyNYSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the TensorFlow model"
      ],
      "metadata": {
        "id": "8LkPcJsSwZaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_model = tf.function(lambda x: model(x))\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "STEPS = x_train.shape[1]\n",
        "FEATURES = x_train.shape[2]\n",
        "\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "    tf.TensorSpec([BATCH_SIZE, STEPS, FEATURES], model.inputs[0].dtype))\n",
        "\n",
        "# model directory.\n",
        "model.save(TF_MODEL, save_format=\"tf\", signatures=concrete_func)"
      ],
      "metadata": {
        "id": "lf_yxWnIwbbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Design a many-to-one LSTM model"
      ],
      "metadata": {
        "id": "PHXbM8bENA_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "input_shape = (x_train.shape[1], x_train.shape[2])\n",
        "\n",
        "norm_layer = layers.Normalization(axis=-1)\n",
        "\n",
        "# Learn mean and standard deviation from dataset\n",
        "norm_layer.adapt(x_train)\n",
        "\n",
        "input = layers.Input(shape=input_shape)\n",
        "x = norm_layer(input)\n",
        "x = layers.LSTM(32, return_sequences=False)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "x = layers.Dense(len(LIST_GENRES), activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(input, x)"
      ],
      "metadata": {
        "id": "EMqlQoJaYb9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the many-to-one LSTM model"
      ],
      "metadata": {
        "id": "xm5kFMDL7DXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "model.compile(optimizer=optimiser,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "NUM_EPOCHS = 30\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_data=(x_validate, y_validate))"
      ],
      "metadata": {
        "id": "qNOw0NDw7FQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the quantized TensorFlow Lite model's accuracy on the test dataset"
      ],
      "metadata": {
        "id": "ivAVJs0oBVu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select a few hundred of samples randomly from the test dataset to calibrate the quantization"
      ],
      "metadata": {
        "id": "AjkkNZrAb-YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def representative_data_gen():\n",
        "  data = tf.data.Dataset.from_tensor_slices(x_test)\n",
        "  for i_value in data.batch(1).take(100):\n",
        "    i_value_f32 = tf.dtypes.cast(i_value, tf.float32)\n",
        "    yield [i_value_f32]"
      ],
      "metadata": {
        "id": "9jqXfw55wiDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantize the TensorFlow model with the TensorFlow Lite converter"
      ],
      "metadata": {
        "id": "7ipKoUlJb5Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(TF_MODEL)\n",
        "converter.representative_dataset = tf.lite.RepresentativeDataset(representative_data_gen)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "tfl_model = converter.convert()"
      ],
      "metadata": {
        "id": "S7p8jsvkwiqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the TensorFlow Lite interpreter"
      ],
      "metadata": {
        "id": "jtczMDIAf0lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the TFLite interpreter\n",
        "interp = tf.lite.Interpreter(model_content=tfl_model)"
      ],
      "metadata": {
        "id": "qYhn95JKf2Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Allocate the tensor and get input quantization parameters"
      ],
      "metadata": {
        "id": "aBJC2smof5u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Allocate the tensors\n",
        "interp.allocate_tensors()\n",
        "\n",
        "# Get input/output layer information\n",
        "i_details = interp.get_input_details()[0]\n",
        "o_details = interp.get_output_details()[0]\n",
        "\n",
        "# Get input quantization parameters.\n",
        "i_quant = i_details[\"quantization_parameters\"]\n",
        "i_scale      = i_quant['scales'][0]\n",
        "i_zero_point = i_quant['zero_points'][0]"
      ],
      "metadata": {
        "id": "CdZVMUKsf-To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement a function to run the model inference using the TensorFlow Lite Python interpreter"
      ],
      "metadata": {
        "id": "0x2y6ajEJnmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(i_value):\n",
        "  # Add an extra dimension to the test sample\n",
        "  # to match the expected 3D tensor shape\n",
        "  i_value_f32 = np.expand_dims(i_value, axis=0)\n",
        "\n",
        "  # Quantize (float -> 8-bit) the input\n",
        "  i_value_f32 = i_value_f32 / i_scale + i_zero_point\n",
        "  i_value_s8 = tf.cast(i_value_f32, dtype=tf.int8)\n",
        "  interp.set_tensor(i_details[\"index\"], i_value_s8)\n",
        "\n",
        "  interp.invoke()\n",
        "\n",
        "  # TfLite fused Lstm kernel is stateful.\n",
        "  # Therefore, we need to reset the states before the next inference\n",
        "  interp.reset_all_variables()\n",
        "\n",
        "  return interp.get_tensor(o_details[\"index\"])[0]"
      ],
      "metadata": {
        "id": "BjX4etm7JuT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the accuracy of the quantized TensorFlow Lite model"
      ],
      "metadata": {
        "id": "DHNNQ925hdnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct_samples = 0\n",
        "\n",
        "for i_value, o_value in zip(x_test, y_test):\n",
        "  o_pred_f32 = classify(i_value)\n",
        "  o_res = np.argmax(o_pred_f32)\n",
        "  if o_res == o_value:\n",
        "    num_correct_samples += 1\n",
        "\n",
        "num_total_samples   = len(x_test)\n",
        "print(\"Accuracy:\", num_correct_samples/num_total_samples)"
      ],
      "metadata": {
        "id": "FWPDjyM3ho6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert the TensorFlow model to C-byte array with xxd"
      ],
      "metadata": {
        "id": "CVKEO66QnXPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "open(TFL_MODEL_FILE, \"wb\").write(tfl_model)"
      ],
      "metadata": {
        "id": "y81Bd6f3neMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get -qq install xxd\n",
        "!xxd -i $TFL_MODEL_FILE > model.h\n",
        "!sed -i 's/unsigned char/const unsigned char/g' model.h"
      ],
      "metadata": {
        "id": "ilSTJDJQnhxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp music_genre_int8.tflite drive/MyDrive/mgr_dataset"
      ],
      "metadata": {
        "id": "oc4NdSZbxVkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model with a song"
      ],
      "metadata": {
        "id": "GO4hYlcxfua8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_expected_class = 'jazz'\n",
        "test_song_link = 'https://cdn.pixabay.com/download/audio/2022/05/11/audio_58647a4eda.mp3?filename=jazzy-jazz-110967.mp3'"
      ],
      "metadata": {
        "id": "RUuVSVS0fxrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_song_name = 'test_' + test_expected_class + '_song.mp3'"
      ],
      "metadata": {
        "id": "AQAsFZxef2aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O {test_song_name} {test_song_link} ."
      ],
      "metadata": {
        "id": "wUlfs9Nvf4_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploying the MFCCs feature extraction algorithm on the Raspberry Pi Pico"
      ],
      "metadata": {
        "id": "62v7vGW5TWwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement a function to transform a NumPy array to C array"
      ],
      "metadata": {
        "id": "92A1mqyJUmQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_c_array(data, c_type, filename, num_cols = 12):\n",
        "\n",
        "  def to_numpy_dt(dtype):\n",
        "    if dtype == 'float':\n",
        "      return 'float32'\n",
        "    if dtype == 'int32_t':\n",
        "      return 'int32'\n",
        "    if dtype == 'uint32_t':\n",
        "      return 'uint32'\n",
        "    if dtype == 'int16_t':\n",
        "      return 'int16'\n",
        "    if dtype == 'uint16_t':\n",
        "      return 'uint16'\n",
        "    if dtype == 'int8_t':\n",
        "      return 'int8'\n",
        "    if dtype == 'uint8_t':\n",
        "      return 'uint8'\n",
        "    return ''\n",
        "\n",
        "  str_out = ''\n",
        "\n",
        "  # Write the header guard\n",
        "  header_guard = filename.upper()\n",
        "  str_out += '#ifndef ' + header_guard + '\\n'\n",
        "  str_out += '#define ' + header_guard + '\\n'\n",
        "\n",
        "  # Write the tensor dimensions\n",
        "  # Scan the dimensions in reverse order\n",
        "  dim_base = 'const int32_t ' + filename + '_dim'\n",
        "  for idx, dim in enumerate(data.shape[::-1]):\n",
        "    str_out += dim_base + str(idx) + ' = '\n",
        "    str_out += str(dim)\n",
        "    str_out += ';\\n'\n",
        "\n",
        "  # Reshape the NumPy array and cast the array to desired C data type\n",
        "  np_type  = to_numpy_dt(c_type)\n",
        "  data_out = data.flatten()\n",
        "  data_out = data_out.astype(np_type)\n",
        "\n",
        "  # Write the tensor total size (Optional)\n",
        "  size = len(data_out)\n",
        "  sz_base = 'const int32_t ' + filename + '_sz'\n",
        "  str_out += sz_base + ' = '\n",
        "  str_out += str(size) + ';\\n'\n",
        "\n",
        "  # Write the array definition\n",
        "  str_out += 'const ' + c_type + ' ' + filename + '_data[] = '\n",
        "  str_out += \"\\n{\\n\"\n",
        "\n",
        "  # Write the values\n",
        "  for i, val in enumerate(data_out):\n",
        "    str_out += str(val)\n",
        "\n",
        "    if (i + 1) < len(data_out):\n",
        "        str_out += ','\n",
        "    if (i + 1) % num_cols == 0:\n",
        "        str_out += '\\n'\n",
        "\n",
        "  str_out += '};\\n'\n",
        "  str_out += '#endif\\n'\n",
        "\n",
        "  # Save the C header file\n",
        "  h_filename = filename + '.h'\n",
        "  open(h_filename, \"w\").write(str_out)"
      ],
      "metadata": {
        "id": "aKe11ZzoU2Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement a function to generate a header file with the constants required by the MFCCs feature extraction algorithm"
      ],
      "metadata": {
        "id": "87GiCFiK8vsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_c_consts(data, filename):\n",
        "  str_out = ''\n",
        "\n",
        "  # Write the header guard\n",
        "  header_guard = filename.upper()\n",
        "  str_out += '#ifndef ' + header_guard + '\\n'\n",
        "  str_out += '#define ' + header_guard + '\\n'\n",
        "\n",
        "  for x in data:\n",
        "    value    = x[0]\n",
        "    var_name = x[1]\n",
        "    c_type   = x[2]\n",
        "    str_out += 'const ' + c_type + ' '\n",
        "    str_out += var_name + ' = '\n",
        "    str_out += str(value)\n",
        "    str_out += ';\\n'\n",
        "\n",
        "  str_out += '#endif\\n'\n",
        "\n",
        "  # Save the C header file\n",
        "  h_filename = filename + '.h'\n",
        "  open(h_filename, \"w\").write(str_out)"
      ],
      "metadata": {
        "id": "Qwk41WUD9GjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate the C arrays for all the precomputed components of the MFCCs feature extraction algorithm:"
      ],
      "metadata": {
        "id": "BygZzFx5AlS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_c_array(hann_lut_q15, 'int16_t', 'hann_lut_q15')\n",
        "to_c_array(mel_wei_mtx_q15.T, 'int16_t', 'mel_wei_mtx_q15_T')\n",
        "to_c_array(log_lut_q13_3, 'int16_t', 'log_lut_q13_3')\n",
        "to_c_array(dct_wei_mtx_q15.T, 'int16_t', 'dct_wei_mtx_q15_T')"
      ],
      "metadata": {
        "id": "EdpG7P39co_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate the C arrays for the int16 input test and its expected MFCCs in floating-point format"
      ],
      "metadata": {
        "id": "hxolzhApD2BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_src_q15 = dsp.arm_float_to_q15(test_ad)\n",
        "\n",
        "to_c_array(test_src_q15, 'int16_t', 'test_src')\n",
        "to_c_array(mfccs_cmsis, 'float', 'test_dst')"
      ],
      "metadata": {
        "id": "RgGDMuBDEZIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate the C constants required by the MFCCs feature extraction algorithm"
      ],
      "metadata": {
        "id": "iWheMYL5HPAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_FRAMES = int(((TRAIN_AUDIO_LENGTH_SAMPLES - FRAME_LENGTH) / FRAME_STEP) + 1)\n",
        "NUM_FFT_FREQS = int((FFT_LENGTH / 2) + 1)\n",
        "\n",
        "vars = [\n",
        "       (FRAME_LENGTH, 'FRAME_LENGTH', 'int32_t'),\n",
        "       (FRAME_STEP, 'FRAME_STEP', 'int32_t'),\n",
        "       (NUM_FRAMES, 'NUM_FRAMES', 'int32_t'),\n",
        "       (FFT_LENGTH, 'FFT_LENGTH', 'int32_t'),\n",
        "       (NUM_FFT_FREQS, 'NUM_FFT_FREQS', 'int32_t'),\n",
        "       (NUM_MEL_FREQS, 'NUM_MEL_FREQS', 'int32_t'),\n",
        "       (NUM_MFCCS, 'NUM_MFCCS', 'int32_t')\n",
        "       ]\n",
        "\n",
        "to_c_consts(vars, 'mfccs_consts')"
      ],
      "metadata": {
        "id": "BWjgBhsYHTJl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}